import pandas as pd
import numpy as np
import cv2
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from itertools import combinations
import missingno
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings('ignore')

# Read the CSV file
kaggle_dataset_path = ('/kaggle/input/iris-flower-dataset/IRIS.csv')
ds = pd.read_csv(kaggle_dataset_path)

# Data exploration
print(ds.species.value_counts())
print(ds.info())
print(ds.isnull().sum().sort_values(ascending=False))
print(ds.shape)
print(ds.describe())
print(ds.nunique())
print(ds.species.unique())

# Data Visualization
figg = px.histogram(data_frame=ds, x='species', title='Total number of species', color='species')
figg.show()

def Histogram(column):
    global ds
    fig = go.Figure()
    fig.add_trace(go.Histogram(x=ds[column], marker_color='skyblue', marker_line_color='black', marker_line_width=1.5))
    fig.update_layout(title=f'Histogram of {column}', xaxis_title=column, yaxis_title='Frequency', template='plotly_dark')
    fig.show()

Histogram('sepal_length')
Histogram('petal_length')
Histogram('sepal_width')

# Additional Data Visualization
plt.subplot(2, 2, 1)
sns.boxplot(ds['sepal_length'], color='red')
plt.tight_layout()
plt.show()

plt.subplot(2, 2, 3)
sns.boxplot(ds['petal_length'], color='purple')
plt.tight_layout()
plt.show()

plt.subplot(2, 2, 4)
sns.boxplot(ds['petal_width'], color='purple')
plt.tight_layout()
plt.show()

fig1 = px.histogram(ds, x="species", color="sepal_length")
fig1.show()

fig2 = px.histogram(ds, x="species", color="sepal_width")
fig2.show()

fig3 = px.histogram(ds, x="species", color="petal_width")
fig3.show()

fig4 = px.histogram(ds, x="species", color="petal_length")
fig4.show()

# Scatter plots
def Scatterplot(x, y, c=None):
    global ds
    plt.figure(figsize=(15, 6))
    for species in ds['species'].unique():
        plt.scatter(ds[x][ds['species'] == species], ds[y][ds['species'] == species], label=species, edgecolor="k", alpha=0.7)
    plt.xticks(rotation=0)
    plt.title("Scatter plot X:{} / Y:{}".format(x, y))
    plt.xlabel(x)
    plt.ylabel(y)
    plt.legend()
    plt.show()

comb = combinations(["species", "petal_length", "petal_width", "sepal_length", "sepal_width"], 2)
comb_list = [list(i) for i in comb]

for col in comb_list:
    Scatterplot(col[0], col[1])

sns.pairplot(ds, hue='species', palette='Dark2')

# Feature Engineering
species_encoding = ds.groupby('species').agg({'sepal_length': 'mean'}).to_dict()
ds['species_encoded'] = round(ds['species'].map(species_encoding['sepal_length']), 1)
ds.drop(['species'], axis=1, inplace=True)

# Data Preprocessing
missingno.bar(ds, color="dodgerblue", sort="ascending", figsize=(6, 3), fontsize=12)
sns.heatmap(ds.corr(method='spearman'), annot=True, cmap="BrBG")

# Model Training and Evaluation
X = ds.drop(['species_encoded'], axis=1)
y = ds['species_encoded']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=32)

# Linear Regression
lr = LinearRegression()
lr.fit(X_train, y_train)
lr_pred_train = lr.predict(X_train)
lr_pred_test = lr.predict(X_test)
print('Linear Regression - Test Metrics:')
print('Mean Squared Error:', mean_squared_error(y_test, lr_pred_test))
print('Mean Absolute Error:', mean_absolute_error(y_test, lr_pred_test))
print('R2 Score:', r2_score(y_test, lr_pred_test))

# RandomForest Regressor
rfg = RandomForestRegressor()
rfg.fit(X_train, y_train)
rf_pred_train = rfg.predict(X_train)
rf_pred_test = rfg.predict(X_test)
print('Random Forest - Test Metrics:')
print('Mean Squared Error:', mean_squared_error(y_test, rf_pred_test))
print('Mean Absolute Error:', mean_absolute_error(y_test, rf_pred_test))
print('R2 Score:', r2_score(y_test, rf_pred_test))

# XGBoost Regressor
xgboost = XGBRegressor(objective='reg:squarederror', random_state=42)
xgboost.fit(X_train, y_train)
y_pred_xgboost_train = xgboost.predict(X_train)
y_pred_xgboost_test = xgboost.predict(X_test)
print('XGBoost - Test Metrics:')
print('Mean Squared Error:', mean_squared_error(y_test, y_pred_xgboost_test))
print('Mean Absolute Error:', mean_absolute_error(y_test, y_pred_xgboost_test))
print('R2 Score:', r2_score(y_test, y_pred_xgboost_test))

# Decision Tree Regressor
dt = DecisionTreeRegressor(max_depth=10, random_state=0)
dt.fit(X_train, y_train)
y_pred_dt_train = dt.predict(X_train)
y_pred_dt_test = dt.predict(X_test)
print('Decision Tree - Test Metrics:')
print('Mean Squared Error:', mean_squared_error(y_test, y_pred_dt_test))
print('Mean Absolute Error:', mean_absolute_error(y_test, y_pred_dt_test))
print('R2 Score:', r2_score(y_test, y_pred_dt_test))

# Evaluation Metrics Comparison
metrics_df = pd.DataFrame({
    'Model': ['Linear Regression', 'Random Forest', 'XGBoost', 'Decision Tree'],
    'MAE Test': [mean_absolute_error(y_test, lr_pred_test),
                 mean_absolute_error(y_test, rf_pred_test),
                 mean_absolute_error(y_test, y_pred_xgboost_test),
                 mean_absolute_error(y_test, y_pred_dt_test)],
    'MSE Test': [mean_squared_error(y_test, lr_pred_test),
                 mean_squared_error(y_test, rf_pred_test),
                 mean_squared_error(y_test, y_pred_xgboost_test),
                 mean_squared_error(y_test, y_pred_dt_test)],
    'R2 Score Test': [r2_score(y_test, lr_pred_test),
                      r2_score(y_test, rf_pred_test),
                      r2_score(y_test, y_pred_xgboost_test),
                      r2_score(y_test, y_pred_dt_test)]
})

# Plotting Evaluation Metrics
fig_metrics = make_subplots(rows=3, cols=1, subplot_titles=('Mean Absolute Error', 'Mean Squared Error', 'R2 Score'))

fig_metrics.add_trace(go.Bar(x=metrics_df['Model'], y=metrics_df['MAE Test'], name='MAE Test', marker_color='blue'), row=1, col=1)
fig_metrics.add_trace(go.Bar(x=metrics_df['Model'], y=metrics_df['MSE Test'], name='MSE Test', marker_color='red'), row=2, col=1)
fig_metrics.add_trace(go.Bar(x=metrics_df['Model'], y=metrics_df['R2 Score Test'], name='R2 Score Test', marker_color='green'), row=3, col=1)

fig_metrics.update_layout(showlegend=False, height=600, title_text="Model Evaluation Metrics")
fig_metrics.show()

# Scatter plots for model evaluation
scatter_color = 'skyblue'
plot_width = 800
plot_height = 275

# Scatter plot for XGBoost Regressor
fig_xgboost_train = px.scatter(x=y_train, y=y_pred_xgboost_train,
                               labels={'x': 'Actual Ratings', 'y': 'Predicted Ratings'},
                               title='Evaluation of Training XGBoost Regressor Model', trendline='ols',
                               color_discrete_sequence=[scatter_color])
fig_xgboost_train.update_layout(showlegend=True, width=plot_width, height=plot_height)

fig_xgboost_test = px.scatter(x=y_test, y=y_pred_xgboost_test,
                              labels={'x': 'Actual Ratings', 'y': 'Predicted Ratings'},
                              title='Evaluation of Testing XGBoost Regressor Model', trendline='ols',
                              color_discrete_sequence=[scatter_color])
fig_xgboost_test.update_layout(showlegend=True, width=plot_width, height=plot_height)

fig_xgboost_train.show()
fig_xgboost_test.show()

# Scatter plots for RandomForest Regressor
fig_rf_train = px.scatter(x=y_train, y=rf_pred_train,
                          labels={'x': 'Actual Ratings', 'y': 'Predicted Ratings'},
                          title='Evaluation of Training RandomForest Regressor Model', trendline='ols',
                          color_discrete_sequence=[scatter_color])
fig_rf_train.update_layout(showlegend=True, width=plot_width, height=plot_height)

fig_rf_test = px.scatter(x=y_test, y=rf_pred_test,
                         labels={'x': 'Actual Ratings', 'y': 'Predicted Ratings'},
                         title='Evaluation of Testing RandomForest Regressor Model', trendline='ols',
                         color_discrete_sequence=[scatter_color])
fig_rf_test.update_layout(showlegend=True, width=plot_width, height=plot_height)

fig_rf_train.show()
fig_rf_test.show()

# Prediction DataFrame for XGBoost Regressor Model
prediction_xgboost = pd.DataFrame({'Actual Species': y_test,
                                    'Predicted Species': y_pred_xgboost_test.ravel(),
                                    'Residual': y_test - y_pred_xgboost_test}
                                  )
print(prediction_xgboost.head(10))

# Thank you!
print("\nThank you!")
print("Made by Mohsin Malik")
